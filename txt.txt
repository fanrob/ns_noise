Расскажи немного об устройстве этой нейросети простыми словам. Как она отличается от персептрона, где происходят сами вычисления состояния сети.

Эта нейросеть — упрощённая версия Трансформера. Вот как она устроена простыми словами:

Как работает сеть
Вход — это последовательность чисел (например, 0, 1, 2, ...). Каждый элемент превращается в вектор (обычно через эмбеддинг, здесь — просто расширение размерности).
Позиционное кодирование — добавляет к каждому вектору информацию о его позиции в последовательности (чтобы сеть понимала порядок).
Слои энкодера — каждый слой состоит из двух частей:
Self-Attention (самовнимание): каждый элемент "смотрит" на другие элементы последовательности и решает, на что обратить внимание. Это делается через специальные матрицы весов (Q, K, V) и функцию внимания.
Feed-Forward слой: обычный полносвязный слой, который обрабатывает каждый элемент по отдельности.
После каждой части есть остаточное соединение (residual): к выходу слоя прибавляется его вход, чтобы облегчить обучение.
Выход — преобразованная последовательность, где каждый элемент содержит информацию о всей последовательности.
Чем отличается от персептрона
Персептрон (или обычная полносвязная сеть) обрабатывает каждый элемент независимо, не учитывая порядок и связи между элементами.
Трансформер использует механизм внимания, чтобы каждый элемент мог учитывать всю последовательность, а не только себя.
Вся "магия" происходит в Self-Attention: здесь вычисляются веса внимания и происходит "смешивание" информации между элементами.
Где происходят вычисления состояния
Веса (матрицы W_q, W_k, W_v, W1, W2) хранятся в каждом слое.
Вычисления происходят при каждом вызове слоя: входные данные умножаются на веса, применяются функции внимания и активации, складываются с остаточными связями.
Состояние сети — это значения на каждом этапе прохождения данных через слои.
Главное отличие:
Трансформер может учитывать взаимосвязи между элементами последовательности, а персептрон — нет.